{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM example: Identifyng Chronic Kidney Disease\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through an example implementing a Support Vector Machine to tackle a medical classification problem. The data comes from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease), based on research by P. Soundarapandian, L. Jerlin Rubini, and P. Esweran.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Implementing a linear SVM\n",
    "1. Applying and tuning the model\n",
    "1. Introducing non-linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using both [NumPy](https://numpy.org/doc/stable/index.html) arrays and [pandas](https://pandas.pydata.org/docs/index.html) DataFrames, so we need to import these packages with their standard abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# global random number generator\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Recall that a **Support Vector Machine** seeks to minimize classification errors while maximizing the width of the margin around the decision boundary, in hopes of increasing generalizability. More formally, we have a dataset of $n$ feature vectors $x^{(i)}$ and labels $y^{(i)}\\in {-1,1}$. We want to find a set of parameters ($\\theta, \\theta_0$) that mimimize the cost function:\n",
    "$$ J(\\theta , \\theta _0) = \\frac{1}{n} \\sum _{i=1}^{n} \\text {Loss}_ h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta _0 )) + \\frac{\\lambda }{2} \\mid \\mid \\theta \\mid \\mid ^2$$\n",
    "where\n",
    "$$\\text{Loss}_h(z)=\\begin{cases}0 & z \\ge 1 \\\\1-z & z < 1\\end{cases}$$\n",
    "and $\\lambda$ is the **regularization parameter**, determining the relative weight given to margin width (i.e. the inverse of the squared norm of $\\theta$) and accuracy (i.e. the `Loss` term). Higher $\\lambda$ focuses more on regularization by widening the margin.\n",
    "\n",
    "We estimate the parameters through **Stochastic Gradient Descent (SGD)**. This means we randomly select an observation $i$ and update the parameters as follows:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla _{\\theta } \\big [\\text {Loss}_ h(y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0) ) + \\frac{\\lambda }{2}\\mid \\mid \\theta \\mid \\mid ^2 \\big ]$$\n",
    "where $\\nabla$ is the learning rate. This can be a constant or can be adjusted over the training process. We will use the latter, based on the formula:\n",
    "$$ \\eta_{t+1} = \\frac {\\eta_t}{t^d} $$\n",
    "where $t$ is the training epoch, $\\eta_t$ is the learning rate for the $t$-th epoch, and $d$ is a decay parameter. \n",
    "\n",
    "If the case is placed correctly and outside the classification margin by the current parameters (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)\\ge 1$), the update is based strictly on the regularization term. So, the gradient is $\\lambda \\theta$.\n",
    "\n",
    "If there is a positive `Loss` (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)<1$), then the gradient is $-y^{(i)}x^{(i)} + \\lambda \\theta$.\n",
    "\n",
    "We continue cycling through the data, shuffling after each epoch, until the change in the cost remains below some threshold.\n",
    "\n",
    "Note that this algorithm only optimizes the parameters $\\theta$, and $\\theta_0$, while $\\lambda$, the initial $\\eta$, the learning rate decay parameter, and the convergence threshold are **hyperparameters** that must be set in advance.\n",
    "\n",
    "NOTE ABOUT THE OFFSET: rather than treating $\\theta_0$ as a separate parameter, it is possibe instead to prepend a `1` to every feature vector, so that the SVM trains $\\theta$ as a $D+1$ vector, where the first entry is equivalent to $\\theta_0$. This is what will be done below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The cell below implements a linear SVM as a class that mimics the syntax of the popular machine learning package [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
    "1. The model is first initialized with given (default or custom) hyperparameters. Here those are the regularization paramter `lam` (for $\\lambda$), the learning rate `lrate`, the `decay` of the learning rate, and the convergence `threshold`.\n",
    "1. Then, the model is trained on data using the method `fit(X, y)`, where X contains the features vectors (observations as rows, features as columns) and y contains the labels. The model saves the trained parameters.\n",
    "1. After that, the model can be used to assign labels to any feature vectors with the method `predict(X)`\n",
    "1. Finally, `score(X, y)` calculates and returns the share of correctly predicted cases.\n",
    "\n",
    "The class below also contains three helper functions called by `predict()`:\n",
    "1. `cost()` calculates the cost function for the entire dataset, needed to check for convergence\n",
    "1. `loss()` calculates and returns the Loss function for calculating the cost function, as defined above\n",
    "1. `train_epoch()` runs through the observations in random order one time, updating the parameters accordingly, and returning the resulting cost function.\n",
    "\n",
    "The `set_params()` and `copy()` methods will be used in hyperparameter tuning, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM():\n",
    "    def __init__(self, lam=1., lrate0=1., decay=0.5, threshold=1e-4) -> None:\n",
    "        '''\n",
    "        Initialize the model with the given hyperparameters:\n",
    "        - `lam`: the regularization parameter lambda\n",
    "        - `lrate`: the learning rate\n",
    "        - `threshold`: the convergence threshold\n",
    "        '''\n",
    "        self.lam = lam\n",
    "        self.lrate0 = lrate0\n",
    "        self.decay = decay\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "        self.max_iter = 1000\n",
    "\n",
    "    def loss(self, Xi, yi):\n",
    "        '''The Loss function for one observation'''\n",
    "        if self.theta is None:\n",
    "            raise ValueError('Cannot calculate loss without initializing theta')\n",
    "\n",
    "        agreement = yi*(self.theta @ Xi)\n",
    "        return 0 if agreement >= 1 else 1-agreement\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        '''Average cost for the whole dataset'''\n",
    "        loss_sum = 0\n",
    "        for Xi, yi in zip(X, y):\n",
    "            loss_sum += self.loss(Xi, yi)\n",
    "        return loss_sum/len(y) + self.lam/2 * (self.theta@self.theta)\n",
    "    \n",
    "    def train_epoch(self, X, y):\n",
    "        '''Cycles through cases in random order, updating theta. Returns the average cost after every update'''\n",
    "        order = np.arange(len(y)) \n",
    "        RNG.shuffle(order)\n",
    "        for i in order:\n",
    "            grad = self.lam * self.theta\n",
    "            grad -= y[i]*X[i] if self.loss(X[i], y[i]) > 0 else 0\n",
    "            self.theta -= self._lrate * grad\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Fits the model to the data (feature vectors X, labels y). To fit with an offset, prepend `1` to every feature vector'''\n",
    "        # initialize theta\n",
    "        self.theta = np.zeros(X.shape[-1])\n",
    "        self._lrate = self.lrate0\n",
    "\n",
    "        last_cost = np.inf\n",
    "        for i in range(self.max_iter):\n",
    "            new_cost = self.train_epoch(X, y)\n",
    "            if np.abs(last_cost - new_cost) < self.threshold:\n",
    "                break\n",
    "            else:\n",
    "                last_cost = new_cost\n",
    "                self._lrate = self._lrate/(1+i)**self.decay\n",
    "        else:\n",
    "            print(f\"Maximum of {self.max_iter} reached without convergence\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predicts labels for feature vectors X'''\n",
    "        return sign(X @ self.theta)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y == y_pred)\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "    \n",
    "    def copy(self):\n",
    "        return LinearSVM(self.lam, self.lrate0, self.decay, self.threshold)\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def sign(z):\n",
    "    '''Converts values to labels, i.e. 1 and -1'''\n",
    "    return 1 if z>=0 else -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying and Tuning the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as in the file `ckd_data.csv`, and we can read it into a pandas DataFrame.\n",
    "\n",
    "The data has 400 rows. However, only the target label (`class`) has a full 400 observations: every feature has at least some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     391 non-null    float64\n",
      " 1   bp      388 non-null    float64\n",
      " 2   sg      353 non-null    float64\n",
      " 3   al      354 non-null    float64\n",
      " 4   su      351 non-null    float64\n",
      " 5   rbc     248 non-null    object \n",
      " 6   pc      335 non-null    object \n",
      " 7   pcc     396 non-null    object \n",
      " 8   ba      396 non-null    object \n",
      " 9   bgr     356 non-null    float64\n",
      " 10  bu      381 non-null    float64\n",
      " 11  sc      383 non-null    float64\n",
      " 12  sod     313 non-null    float64\n",
      " 13  pot     312 non-null    float64\n",
      " 14  hemo    348 non-null    float64\n",
      " 15  pcv     329 non-null    float64\n",
      " 16  wbcc    294 non-null    float64\n",
      " 17  rbcc    269 non-null    float64\n",
      " 18  htn     398 non-null    object \n",
      " 19  dm      398 non-null    object \n",
      " 20  cad     398 non-null    object \n",
      " 21  appet   399 non-null    object \n",
      " 22  pe      399 non-null    object \n",
      " 23  ane     399 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 81.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('ckd_data.csv', index_col=0)\n",
    "print(raw_data.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Model\n",
    "\n",
    "As a first pass, we can adopt the most conservative approach to the missing values: dropping any row that contained any. This results in 158 valid observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 3 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     158 non-null    float64\n",
      " 1   bp      158 non-null    float64\n",
      " 2   sg      158 non-null    float64\n",
      " 3   al      158 non-null    float64\n",
      " 4   su      158 non-null    float64\n",
      " 5   rbc     158 non-null    object \n",
      " 6   pc      158 non-null    object \n",
      " 7   pcc     158 non-null    object \n",
      " 8   ba      158 non-null    object \n",
      " 9   bgr     158 non-null    float64\n",
      " 10  bu      158 non-null    float64\n",
      " 11  sc      158 non-null    float64\n",
      " 12  sod     158 non-null    float64\n",
      " 13  pot     158 non-null    float64\n",
      " 14  hemo    158 non-null    float64\n",
      " 15  pcv     158 non-null    float64\n",
      " 16  wbcc    158 non-null    float64\n",
      " 17  rbcc    158 non-null    float64\n",
      " 18  htn     158 non-null    object \n",
      " 19  dm      158 non-null    object \n",
      " 20  cad     158 non-null    object \n",
      " 21  appet   158 non-null    object \n",
      " 22  pe      158 non-null    object \n",
      " 23  ane     158 non-null    object \n",
      " 24  class   158 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_dropNA = raw_data.dropna()\n",
    "data_dropNA.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the non-numeric columns (dtype object above) into a numeric format. As shown below, tt turns out that each of these features represent binary categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1\n",
      "rbc        normal    abnormal\n",
      "pc       abnormal      normal\n",
      "pcc       present  notpresent\n",
      "ba     notpresent     present\n",
      "htn           yes          no\n",
      "dm             no         yes\n",
      "cad            no         yes\n",
      "appet        poor        good\n",
      "pe            yes          no\n",
      "ane           yes          no\n",
      "class         ckd      notckd\n"
     ]
    }
   ],
   "source": [
    "print(data_dropNA.select_dtypes('object').apply(pd.unique).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can easily use binary encoding: replacing the categorical feature with that is 1 if the case falls within one category, otherwise 0. This can be done with the pandas method `.getdummies()`. Note that the parameter `drop_first=True` ensures that only one binary feature is created for each categorical feature.\n",
    "\n",
    "For the target label, we need to recode for the SVM algorithm: the positive label $+1$ applies to patients with chronic kidney disease, and the negative label $-1$ to those who do not.\n",
    "\n",
    "Finally, while the single pandas DataFrame is convenient to exploring the data, for training the model we can split it into feature vectors (X) and labels (y). We also add a 1 to each feature vector to act as an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame):\n",
    "    '''Encoded categorical data numerically and returns separate feature vectors and labels'''\n",
    "    df_copy = df.copy()\n",
    "    df_copy['offset'] = 1\n",
    "    X = pd.get_dummies(df_copy.drop(columns='class'), drop_first=True).to_numpy()\n",
    "    y = np.where(df_copy['class'] == 'ckd', 1., -1.)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(data_dropNA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not yet ready to train the model, however. We need to split our data into training and test sets. The training data is used to train the model, while the test data is used to assess its generalizable performance. We will set aside $\\frac{1}{4}$ of the latter as the test data. This is done randomly, with one adjustment: we want to preserve the rough balance of positively and negatively labeled cases in the training and testing data.\n",
    "\n",
    "We thus will end up with 4 numpy arrays: `X_train`, `X_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y:np.ndarray, train_size:float):\n",
    "    '''Randomly splits arrays into training and test sets, stratifying on the values of `y`'''\n",
    "    positive_indices = np.flatnonzero(y == 1)\n",
    "    RNG.shuffle(positive_indices)\n",
    "    n_train_positive = int(train_size * len(positive_indices))\n",
    "\n",
    "    negative_indices = np.flatnonzero(y == -1)\n",
    "    RNG.shuffle(negative_indices)\n",
    "    n_train_negative = int(train_size * len(negative_indices))\n",
    "\n",
    "    train_indices = np.concatenate(\n",
    "        [\n",
    "            positive_indices[:n_train_positive], \n",
    "            negative_indices[:n_train_negative]\n",
    "        ], \n",
    "        axis=None)\n",
    "    test_indices = np.concatenate(\n",
    "        [\n",
    "            positive_indices[n_train_positive:], \n",
    "            negative_indices[n_train_negative:]\n",
    "        ], \n",
    "        axis=None)\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 25) (40, 25) (118,) (40,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.75)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note here is that between dropping missing values and splitting the data, we are left with rather few observations.\n",
    "\n",
    "The cells below fit the model and report on its classification performance in three ways:\n",
    "1. The share of correctly labeled observations in the training data.\n",
    "1. The share of correctly labeled observations in the testing data.\n",
    "1. More fine-grained measures, broken down by label, for the testing data. Precision is the share of predictions that are correct, while recall is the share of members of the class that are correctly labeled by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training accuracy:  0.9830508474576272\n",
      "Average test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "default_svm = LinearSVM()\n",
    "default_svm.fit(X_train, y_train)\n",
    "print('Average training accuracy: ', default_svm.score(X_train, y_train))\n",
    "print('Average test accuracy: ', default_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      1.00      1.00        29\n",
      "         1.0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = default_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note here:\n",
    "1. Because the labels are imbalanced (i.e. there are many more negative than positive cases), the average accuracy does not fully capture the performance across the two labels. In particular, the precision and recall for the positive cases is usually noticeably lower.\n",
    "1. With so little data, the particular random split between training and testing data substantially impacts the model. This is demonstrated in the next cell, which compares the score for 1000 different iterations of spltting the data and fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reps = 1000\n",
    "scores = np.empty(reps)\n",
    "for i in range(reps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, 0.75)\n",
    "    default_svm = LinearSVM()\n",
    "    default_svm.fit(X_train, y_train)\n",
    "    scores[i] = default_svm.score(X_test, y_test)\n",
    "sns.histplot(scores)\n",
    "plt.title(f'Distribution of scores for {reps} different random test-train splits');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used only one set of hyperparameters (defined as defaults in the initialization function for the model class above): `lam=1., lrate=1., decay=0.5, threshold=1e-4`. How does the performance of the model vary with these hyperparameters? We can answer this question with **k-fold cross-validation**. That is to say, we split the *training* data into a certain number of equal-sized parts (say, 5), then fit train the model 5 times, each with $\\frac {4}{5}$ of the data, and average the performance on the remaining $\\frac{1}{5}$. Then, by comparing the results for different sets of hyperparameters, we can tune the model.\n",
    "\n",
    " Unfortunately, with only 118 cases in the training data, trying five-fold cross validation would result in training on less than 100 cases and validating on 24. We can't afford to just discard the cases with missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ a simple strategy for filling the missing values:\n",
    "1. For numeric features, replace all missing values with the mean.\n",
    "1. For categorical features, replace all missing values with the mode.\n",
    "\n",
    "The cell below does this, taking advantage of the DataFrame method `.select_dtypes()`, where `'object'` corresponds to the categorical data and `'float'` to the numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     400 non-null    float64\n",
      " 1   bp      400 non-null    float64\n",
      " 2   sg      400 non-null    float64\n",
      " 3   al      400 non-null    float64\n",
      " 4   su      400 non-null    float64\n",
      " 5   rbc     400 non-null    object \n",
      " 6   pc      400 non-null    object \n",
      " 7   pcc     400 non-null    object \n",
      " 8   ba      400 non-null    object \n",
      " 9   bgr     400 non-null    float64\n",
      " 10  bu      400 non-null    float64\n",
      " 11  sc      400 non-null    float64\n",
      " 12  sod     400 non-null    float64\n",
      " 13  pot     400 non-null    float64\n",
      " 14  hemo    400 non-null    float64\n",
      " 15  pcv     400 non-null    float64\n",
      " 16  wbcc    400 non-null    float64\n",
      " 17  rbcc    400 non-null    float64\n",
      " 18  htn     400 non-null    object \n",
      " 19  dm      400 non-null    object \n",
      " 20  cad     400 non-null    object \n",
      " 21  appet   400 non-null    object \n",
      " 22  pe      400 non-null    object \n",
      " 23  ane     400 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 81.2+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_fill = raw_data.select_dtypes('object').mode().iloc[0]\n",
    "num_fill = raw_data.select_dtypes('float').mean().iloc[0]\n",
    "data_filled = raw_data.fillna(cat_fill)\n",
    "data_filled.fillna(num_fill, inplace=True)\n",
    "data_filled.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then fit and assess this model using the same procedure as above, we see that the results are somewhat worse. This is not surprising: the cost of including information from the observations with missing values is to introduce some noise from filling in those missing values with the means and modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8662207357859532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.79      0.79      0.79        38\n",
      "         1.0       0.87      0.87      0.87        63\n",
      "\n",
      "    accuracy                           0.84       101\n",
      "   macro avg       0.83      0.83      0.83       101\n",
      "weighted avg       0.84      0.84      0.84       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_data(data_filled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.75)\n",
    "svm = LinearSVM()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Training accuracy: ', svm.score(X_train, y_train))\n",
    "y_pred = svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "With this somehwat larger dataset, we can now try to tune the hyperparameters. There are various strategies for hyperparameter turning, including grid search, random search, and Bayesian optimization. Since our model and data are quite small, we can use the first: this entails listing out alternative values for each hyperparameters and then fitting every possible combination of those values.\n",
    "\n",
    "The next cell defines a class `GridSearchCV` that implements cross-validation by grid-search on a given set of hyperparameters. It mimics the [class of the same name](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) in the Scikit-Learn package.\n",
    "\n",
    "On initialization the `GridSearchCV` takes a model instance (`estimator`, which needs `fit`, `predict`, and `score` methods) and a dict of parameters to search (`param_grid`). In the parameter grid dict, the keys are the parameters names for the model and the values are lists containing the alternative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearchCV():\n",
    "    def __init__(self, estimator, param_grid:dict, cv=5) -> None:\n",
    "        self.base_estimator  = estimator.copy()\n",
    "        self.param_grid = param_grid\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # partition the data\n",
    "        partitions = self.make_partition(len(y))\n",
    "\n",
    "        # initialize results\n",
    "        results = {\n",
    "            'params': [],\n",
    "            'mean_score': []\n",
    "        }\n",
    "        results.update({\n",
    "            f'score{i+1}': [] for i in range(self.cv)\n",
    "        })\n",
    "\n",
    "        # loop through each parameter set, fitting 5 times and saving scores\n",
    "        for param_dict in self.iter_params():\n",
    "            results['params'].append(param_dict)\n",
    "            model = self.base_estimator.copy()\n",
    "            model.set_params(**param_dict)\n",
    "            total_score = 0\n",
    "            for i, (train_set, validation_set) in enumerate(partitions):\n",
    "                model.fit(X[train_set], y[train_set])\n",
    "                score = model.score(X[validation_set], y[validation_set])\n",
    "                results[f'score{i+1}'].append(score)\n",
    "                total_score += score\n",
    "            results['mean_score'].append(total_score/self.cv)\n",
    "        # save results as a DataFrame\n",
    "        self.results = pd.DataFrame(results)\n",
    "        # save best-performing paramets and fit estimator on it with all training data\n",
    "        self.best_params = self.results.sort_values('mean_score', ascending=False).loc[0, 'params']\n",
    "        self.best_estimator = self.base_estimator.copy().set_params(**self.best_params).fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def make_partition(self, length):\n",
    "        '''An iterator that returns training and validation indices for k-fold cross validation'''\n",
    "        indices = np.arange(length)\n",
    "        RNG.shuffle(indices)\n",
    "        step = length // self.cv\n",
    "        start_points = [step*i for i in range(self.cv)]\n",
    "        end_points = [step*i for i in range(1, self.cv+1)]\n",
    "        partitions = []\n",
    "        for start, end in zip(start_points, end_points):\n",
    "            partitions.append((np.concatenate([indices[:start], indices[end:]]), indices[start:end]))\n",
    "        return partitions\n",
    "    \n",
    "    def iter_params(self):\n",
    "        '''Yields every combination of parameters from the parameter dict'''\n",
    "        # d = 0\n",
    "        # perms = 1\n",
    "        # for param_list in self.param_grid.values():\n",
    "        #     d += 1\n",
    "        #     perms *= len(param_list)\n",
    "        # full_grid = [{} for _ in range(perms)]\n",
    "        grid = [{}]\n",
    "        for param_name, param_list in self.param_grid.items():\n",
    "            old_grid = grid\n",
    "            grid = []\n",
    "            for val in param_list:\n",
    "                grid_part = [param_dict.copy() for param_dict in old_grid]\n",
    "                for param_dict in grid_part:\n",
    "                    param_dict.update({param_name:val})\n",
    "                grid += grid_part\n",
    "        for param_dict in grid:\n",
    "            yield param_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 params  mean_score\n",
      "35    {'lam': 0.6000000000000001, 'lrate0': 0.2, 'de...    0.874576\n",
      "93            {'lam': 1.1, 'lrate0': 0.4, 'decay': 0.1}    0.874576\n",
      "138   {'lam': 0.9500000000000001, 'lrate0': 0.600000...    0.871186\n",
      "17            {'lam': 0.9, 'lrate0': 0.1, 'decay': 0.1}    0.871186\n",
      "52    {'lam': 0.25, 'lrate0': 0.30000000000000004, '...    0.871186\n",
      "38           {'lam': 0.75, 'lrate0': 0.2, 'decay': 0.1}    0.871186\n",
      "90    {'lam': 0.9500000000000001, 'lrate0': 0.4, 'de...    0.871186\n",
      "1193          {'lam': 0.9, 'lrate0': 1.0, 'decay': 0.5}    0.871186\n",
      "26    {'lam': 0.15000000000000002, 'lrate0': 0.2, 'd...    0.871186\n",
      "124   {'lam': 0.25, 'lrate0': 0.6000000000000001, 'd...    0.871186\n",
      "1095  {'lam': 0.8, 'lrate0': 0.6000000000000001, 'de...    0.871186\n",
      "47    {'lam': 1.2000000000000002, 'lrate0': 0.2, 'de...    0.871186\n",
      "48    {'lam': 0.05, 'lrate0': 0.30000000000000004, '...    0.871186\n",
      "194   {'lam': 0.15000000000000002, 'lrate0': 0.9, 'd...    0.871186\n",
      "256   {'lam': 0.8500000000000001, 'lrate0': 0.1, 'de...    0.871186\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'lam': [0.05*i for i in range(1, 25)],\n",
    "    'lrate0': [0.1*i for i in range(1, 11)],\n",
    "    'decay': [0.1*i for i in range(1, 11)]\n",
    "}\n",
    "\n",
    "searcher = GridSearchCV(LinearSVM(), param_dict)\n",
    "searcher.fit(X_train, y_train)\n",
    "print(searcher.results[['params', 'mean_score']].sort_values('mean_score', ascending=False).head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
