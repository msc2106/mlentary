{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM example: Identifyng Chronic Kidney Disease\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through an example implementing a Support Vector Machine to tackle a medical classification problem. The data comes from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease), based on research by P. Soundarapandian, L. Jerlin Rubini, and P. Esweran.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Implementing a linear SVM\n",
    "1. Applying and tuning the model\n",
    "1. Implementing and applying kernel SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Recall that a **Support Vector Machine** seeks to minimize classification errors while maximizing the width of the margin around the decision boundary, in hopes of increasing generalizability. More formally, we have a dataset of $n$ feature vectors $x^{(i)}$ and labels $y^{(i)}\\in {-1,1}$. We want to find a set of parameters ($\\theta, \\theta_0$) that mimimize the cost function:\n",
    "$$ J(\\theta , \\theta _0) = \\frac{1}{n} \\sum _{i=1}^{n} \\text {Loss}_ h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta _0 )) + \\frac{\\lambda }{2} \\mid \\mid \\theta \\mid \\mid ^2$$\n",
    "where\n",
    "$$\\text{Loss}_h(z)=\\begin{cases}0 & z \\ge 1 \\\\1-z & z < 1\\end{cases}$$\n",
    "and $\\lambda$ is the **regularization parameter**, determining the relative weight given to margin width (i.e. the inverse of the squared norm of $\\theta$) and accuracy (i.e. the `Loss` term). Higher $\\lambda$ focuses more on regularization by widening the margin.\n",
    "\n",
    "We estimate the parameters through **Stochastic Gradient Descent (SGD)**. This means we randomly select an observation $i$ and update the parameters as follows:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla _{\\theta } \\big [\\text {Loss}_ h(y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0) ) + \\frac{\\lambda }{2}\\mid \\mid \\theta \\mid \\mid ^2 \\big ]$$\n",
    "where $\\nabla$ is the learning rate. This can be a constant or can be adjusted over the training process.\n",
    "\n",
    "If the case is placed correctly and outside the classification margin by the current parameters (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)\\ge 1$), the update is based strictly on the regularization term. So, the gradient is $\\lambda \\theta$.\n",
    "\n",
    "If there is a positive `Loss` (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)<1$), then the gradient is $-y^{(i)}x^{(i)} + \\lambda \\theta$.\n",
    "\n",
    "We continue cycling through the data, shuffling after each epoch, until the change in the cost remains below some threshold.\n",
    "\n",
    "Note that this algorithm only optimizes the parameters $\\theta$, and $\\theta_0$, while $\\lambda$, $\\nabla$, and the convergence threshold are **hyperparameters** that must be set in advance.\n",
    "\n",
    "NOTE ABOUT THE OFFSET: rather than treating $\\theta_0$ as a separate parameter, it is possibe instead to prepend a `1` to every feature vector, so that the SVM trains $\\theta$ as a $D+1$ vector, where the first entry is equivalent to $\\theta_0$. This is what will be done below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The cell below implements a linear SVM as a class that mimics the syntax of the popular machine learning package [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
    "1. The model is first initialized with given (default or custom) hyperparameters. Here those are the regularization paramter `lam` (for $\\lambda$), the learning rate `lrate`, and the convergence `threshold`.\n",
    "1. Then, the model is trained on data using the method `.fit(X, y)`, where X contains the features vectors (observations as rows, features as columns) and y contains the labels. The model saves the trained parameters.\n",
    "1. After that, the model can be used to assign labels to any feature vectors with the method `.predict(X)`\n",
    "\n",
    "The class below also contains three helper functions called by `.predict()`:\n",
    "1. `.cost()` calculates the cost function for the entire dataset, needed to check for convergence\n",
    "1. `.loss()` calculates and returns the Loss function for calculating the cost function, again as defined above\n",
    "1. `.train_epoch()` runs through the observations in random order one time, updating the parameters accordingly, and returning the resulting cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM():\n",
    "    def __init__(self, lam=1, lrate=0.9, threshold=1e-4) -> None:\n",
    "        '''\n",
    "        Initialize the model with the given hyperparameters:\n",
    "        - `lam`: the regularization parameter lambda\n",
    "        - `lrate`: the learning rate\n",
    "        - `threshold`: the convergence threshold\n",
    "        '''\n",
    "        self.lam = lam\n",
    "        self.lrate = lrate\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "\n",
    "    def loss(self, Xi, yi):\n",
    "        '''The Loss function for one observation'''\n",
    "        if self.theta is None:\n",
    "            raise ValueError('Cannot calculate loss without initializing theta')\n",
    "\n",
    "        agreement = yi*(self.theta @ Xi)\n",
    "        return 0 if agreement >= 1 else 1-agreement\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        '''Average cost for the whole dataset'''\n",
    "        loss_sum = 0\n",
    "        for Xi, yi in zip(X, y):\n",
    "            loss_sum += self.loss(Xi, yi)\n",
    "        return loss_sum/len(y) + self.lam/2 * (self.theta@self.theta)\n",
    "    \n",
    "    def train_epoch(self, X, y):\n",
    "        '''Cycles through cases in random order, updating theta. Returns the average cost after every update'''\n",
    "        rng = np.random.default_rng()\n",
    "        order = rng.shuffle(np.arange(len(y)))\n",
    "        for i in order:\n",
    "            grad = self.lam * self.theta\n",
    "            grad -= y[i]*X[i] if self.loss(X[i], y[i]) > 0 else 0\n",
    "            self.theta -= self.lrate * grad\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Fits the model to the data (feature vectors X, labels y). To fit with an offset, prepend `1` to every feature vector'''\n",
    "        # initialize theta\n",
    "        self.theta = np.zeros(X.shape[-1])\n",
    "\n",
    "        last_cost = np.inf\n",
    "        new_cost = 0\n",
    "        while last_cost - new_cost > self.threshold:\n",
    "            new_cost = self.train_epoch(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predicts labels for feature vectors X'''\n",
    "        return sign(X @ self.theta)\n",
    "\n",
    "@np.vectorize\n",
    "def sign(z):\n",
    "    '''Converts values to labels, i.e. 1 and -1'''\n",
    "    return 1 if z>=0 else -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying and Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
