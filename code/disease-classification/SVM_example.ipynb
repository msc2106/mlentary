{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM example: Identifyng Chronic Kidney Disease\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through an example implementing a Support Vector Machine to tackle a medical classification problem. The data comes from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease), based on research by P. Soundarapandian, L. Jerlin Rubini, and P. Esweran.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Implementing a linear SVM\n",
    "1. Applying and tuning the model\n",
    "1. Implementing and applying kernel SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using both [NumPy](https://numpy.org/doc/stable/index.html) arrays and [pandas](https://pandas.pydata.org/docs/index.html) DataFrames, so we need to import these packages with their standard abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Recall that a **Support Vector Machine** seeks to minimize classification errors while maximizing the width of the margin around the decision boundary, in hopes of increasing generalizability. More formally, we have a dataset of $n$ feature vectors $x^{(i)}$ and labels $y^{(i)}\\in {-1,1}$. We want to find a set of parameters ($\\theta, \\theta_0$) that mimimize the cost function:\n",
    "$$ J(\\theta , \\theta _0) = \\frac{1}{n} \\sum _{i=1}^{n} \\text {Loss}_ h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta _0 )) + \\frac{\\lambda }{2} \\mid \\mid \\theta \\mid \\mid ^2$$\n",
    "where\n",
    "$$\\text{Loss}_h(z)=\\begin{cases}0 & z \\ge 1 \\\\1-z & z < 1\\end{cases}$$\n",
    "and $\\lambda$ is the **regularization parameter**, determining the relative weight given to margin width (i.e. the inverse of the squared norm of $\\theta$) and accuracy (i.e. the `Loss` term). Higher $\\lambda$ focuses more on regularization by widening the margin.\n",
    "\n",
    "We estimate the parameters through **Stochastic Gradient Descent (SGD)**. This means we randomly select an observation $i$ and update the parameters as follows:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla _{\\theta } \\big [\\text {Loss}_ h(y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0) ) + \\frac{\\lambda }{2}\\mid \\mid \\theta \\mid \\mid ^2 \\big ]$$\n",
    "where $\\nabla$ is the learning rate. This can be a constant or can be adjusted over the training process.\n",
    "\n",
    "If the case is placed correctly and outside the classification margin by the current parameters (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)\\ge 1$), the update is based strictly on the regularization term. So, the gradient is $\\lambda \\theta$.\n",
    "\n",
    "If there is a positive `Loss` (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)<1$), then the gradient is $-y^{(i)}x^{(i)} + \\lambda \\theta$.\n",
    "\n",
    "We continue cycling through the data, shuffling after each epoch, until the change in the cost remains below some threshold.\n",
    "\n",
    "Note that this algorithm only optimizes the parameters $\\theta$, and $\\theta_0$, while $\\lambda$, $\\nabla$, and the convergence threshold are **hyperparameters** that must be set in advance.\n",
    "\n",
    "NOTE ABOUT THE OFFSET: rather than treating $\\theta_0$ as a separate parameter, it is possibe instead to prepend a `1` to every feature vector, so that the SVM trains $\\theta$ as a $D+1$ vector, where the first entry is equivalent to $\\theta_0$. This is what will be done below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The cell below implements a linear SVM as a class that mimics the syntax of the popular machine learning package [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
    "1. The model is first initialized with given (default or custom) hyperparameters. Here those are the regularization paramter `lam` (for $\\lambda$), the learning rate `lrate`, and the convergence `threshold`.\n",
    "1. Then, the model is trained on data using the method `.fit(X, y)`, where X contains the features vectors (observations as rows, features as columns) and y contains the labels. The model saves the trained parameters.\n",
    "1. After that, the model can be used to assign labels to any feature vectors with the method `.predict(X)`\n",
    "1. Finally, `.score(X, y)` calculates and returns the share of correctly predicted cases.\n",
    "\n",
    "The class below also contains three helper functions called by `.predict()`:\n",
    "1. `.cost()` calculates the cost function for the entire dataset, needed to check for convergence\n",
    "1. `.loss()` calculates and returns the Loss function for calculating the cost function, again as defined above\n",
    "1. `.train_epoch()` runs through the observations in random order one time, updating the parameters accordingly, and returning the resulting cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM():\n",
    "    def __init__(self, lam=0.5, lrate=1., threshold=1e-7) -> None:\n",
    "        '''\n",
    "        Initialize the model with the given hyperparameters:\n",
    "        - `lam`: the regularization parameter lambda\n",
    "        - `lrate`: the learning rate\n",
    "        - `threshold`: the convergence threshold\n",
    "        '''\n",
    "        self.lam = lam\n",
    "        self.lrate = lrate\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "        self.max_iter = 1000\n",
    "\n",
    "    def loss(self, Xi, yi):\n",
    "        '''The Loss function for one observation'''\n",
    "        if self.theta is None:\n",
    "            raise ValueError('Cannot calculate loss without initializing theta')\n",
    "\n",
    "        agreement = yi*(self.theta @ Xi)\n",
    "        return 0 if agreement >= 1 else 1-agreement\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        '''Average cost for the whole dataset'''\n",
    "        loss_sum = 0\n",
    "        for Xi, yi in zip(X, y):\n",
    "            loss_sum += self.loss(Xi, yi)\n",
    "        return loss_sum/len(y) + self.lam/2 * (self.theta@self.theta)\n",
    "    \n",
    "    def train_epoch(self, X, y):\n",
    "        '''Cycles through cases in random order, updating theta. Returns the average cost after every update'''\n",
    "        order = np.arange(len(y)) \n",
    "        RNG.shuffle(order)\n",
    "        for i in order:\n",
    "            grad = self.lam * self.theta\n",
    "            grad -= y[i]*X[i] if self.loss(X[i], y[i]) > 0 else 0\n",
    "            self.theta -= self.lrate * grad\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Fits the model to the data (feature vectors X, labels y). To fit with an offset, prepend `1` to every feature vector'''\n",
    "        # initialize theta\n",
    "        self.theta = np.zeros(X.shape[-1])\n",
    "\n",
    "        last_cost = np.inf\n",
    "        for i in range(self.max_iter):\n",
    "            new_cost = self.train_epoch(X, y)\n",
    "            if np.abs(last_cost - new_cost) < self.threshold:\n",
    "                break\n",
    "            else:\n",
    "                last_cost = new_cost\n",
    "                self.lrate = self.lrate/(1+i)\n",
    "        else:\n",
    "            print(f\"Maximum of {self.max_iter} reached without convergence\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predicts labels for feature vectors X'''\n",
    "        return sign(X @ self.theta)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y == y_pred)\n",
    "\n",
    "@np.vectorize\n",
    "def sign(z):\n",
    "    '''Converts values to labels, i.e. 1 and -1'''\n",
    "    return 1 if z>=0 else -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying and Tuning the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as in the file `ckd_data.csv`, and we can read it into a pandas DataFrame.\n",
    "\n",
    "The data has 400 rows. However, only the target label (`class`) has a full 400 observations: every feature has at least some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     391 non-null    float64\n",
      " 1   bp      388 non-null    float64\n",
      " 2   sg      353 non-null    float64\n",
      " 3   al      354 non-null    float64\n",
      " 4   su      351 non-null    float64\n",
      " 5   rbc     248 non-null    object \n",
      " 6   pc      335 non-null    object \n",
      " 7   pcc     396 non-null    object \n",
      " 8   ba      396 non-null    object \n",
      " 9   bgr     356 non-null    float64\n",
      " 10  bu      381 non-null    float64\n",
      " 11  sc      383 non-null    float64\n",
      " 12  sod     313 non-null    float64\n",
      " 13  pot     312 non-null    float64\n",
      " 14  hemo    348 non-null    float64\n",
      " 15  pcv     329 non-null    float64\n",
      " 16  wbcc    294 non-null    float64\n",
      " 17  rbcc    269 non-null    float64\n",
      " 18  htn     398 non-null    object \n",
      " 19  dm      398 non-null    object \n",
      " 20  cad     398 non-null    object \n",
      " 21  appet   399 non-null    object \n",
      " 22  pe      399 non-null    object \n",
      " 23  ane     399 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 81.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('ckd_data.csv', index_col=0)\n",
    "print(raw_data.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first pass, we can adopt the most conservative approach to the missing values: dropping any row that contained any. This results in 158 valid observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 3 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     158 non-null    float64\n",
      " 1   bp      158 non-null    float64\n",
      " 2   sg      158 non-null    float64\n",
      " 3   al      158 non-null    float64\n",
      " 4   su      158 non-null    float64\n",
      " 5   rbc     158 non-null    object \n",
      " 6   pc      158 non-null    object \n",
      " 7   pcc     158 non-null    object \n",
      " 8   ba      158 non-null    object \n",
      " 9   bgr     158 non-null    float64\n",
      " 10  bu      158 non-null    float64\n",
      " 11  sc      158 non-null    float64\n",
      " 12  sod     158 non-null    float64\n",
      " 13  pot     158 non-null    float64\n",
      " 14  hemo    158 non-null    float64\n",
      " 15  pcv     158 non-null    float64\n",
      " 16  wbcc    158 non-null    float64\n",
      " 17  rbcc    158 non-null    float64\n",
      " 18  htn     158 non-null    object \n",
      " 19  dm      158 non-null    object \n",
      " 20  cad     158 non-null    object \n",
      " 21  appet   158 non-null    object \n",
      " 22  pe      158 non-null    object \n",
      " 23  ane     158 non-null    object \n",
      " 24  class   158 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_dropNA = raw_data.dropna()\n",
    "data_dropNA.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the non-numeric columns (dtype object above) into a numeric format. It turns out that each of these features represent binary categories. So, we can easily use binary encoding: replacing the categorical feature with that is 1 if the case falls within one category, otherwise 0. This can be done with the pandas method `.getdummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rbc        pc         pcc          ba  htn   dm  cad appet   pe  ane  \\\n",
      "0    normal  abnormal     present  notpresent  yes   no   no  poor  yes  yes   \n",
      "1  abnormal    normal  notpresent     present   no  yes  yes  good   no   no   \n",
      "\n",
      "    class  \n",
      "0     ckd  \n",
      "1  notckd  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 3 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   age           158 non-null    float64\n",
      " 1   bp            158 non-null    float64\n",
      " 2   sg            158 non-null    float64\n",
      " 3   al            158 non-null    float64\n",
      " 4   su            158 non-null    float64\n",
      " 5   bgr           158 non-null    float64\n",
      " 6   bu            158 non-null    float64\n",
      " 7   sc            158 non-null    float64\n",
      " 8   sod           158 non-null    float64\n",
      " 9   pot           158 non-null    float64\n",
      " 10  hemo          158 non-null    float64\n",
      " 11  pcv           158 non-null    float64\n",
      " 12  wbcc          158 non-null    float64\n",
      " 13  rbcc          158 non-null    float64\n",
      " 14  rbc_normal    158 non-null    uint8  \n",
      " 15  pc_normal     158 non-null    uint8  \n",
      " 16  pcc_present   158 non-null    uint8  \n",
      " 17  ba_present    158 non-null    uint8  \n",
      " 18  htn_yes       158 non-null    uint8  \n",
      " 19  dm_yes        158 non-null    uint8  \n",
      " 20  cad_yes       158 non-null    uint8  \n",
      " 21  appet_poor    158 non-null    uint8  \n",
      " 22  pe_yes        158 non-null    uint8  \n",
      " 23  ane_yes       158 non-null    uint8  \n",
      " 24  class_notckd  158 non-null    uint8  \n",
      "dtypes: float64(14), uint8(11)\n",
      "memory usage: 20.2 KB\n"
     ]
    }
   ],
   "source": [
    "print(data_dropNA.select_dtypes('object').apply(pd.unique))\n",
    "data_dropNA = pd.get_dummies(data_dropNA, drop_first=True)\n",
    "data_dropNA.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the label, we can rename and recode for easier interpretability: the positive label $+1$ applies to patients with chronic kidney disease, and the negative label $-1$ to those who do not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    115\n",
       " 1     43\n",
       "Name: is_ckd, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dropNA.rename(columns={'class_notckd':'is_ckd'}, inplace=True)\n",
    "data_dropNA['is_ckd'] = data_dropNA.is_ckd*-2 + 1\n",
    "data_dropNA.is_ckd.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to separate the feature vectors from the labels and split our data into training and test sets. The training data is used to train the model, the test data is used to assess its generalizable performance. We will set aside 20% of the latter as the test data. This is done randomly, with one adjustment: we want to preserve the rough balance of positively and negatively labeled cases in the training and testing data.\n",
    "\n",
    "We thus will end up with 4 numpy arrays: `X_train`, `X_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 24) (126,) (32, 24) (32,)\n"
     ]
    }
   ],
   "source": [
    "X = data_dropNA.drop(columns='is_ckd').to_numpy()\n",
    "y = data_dropNA.is_ckd.to_numpy()\n",
    "\n",
    "train_share = 0.8\n",
    "\n",
    "positive_indices = np.flatnonzero(y == 1)\n",
    "RNG.shuffle(positive_indices)\n",
    "n_train_positive = int(train_share * positive_indices.shape[0])\n",
    "\n",
    "negative_indices = np.flatnonzero(y == -1)\n",
    "RNG.shuffle(negative_indices)\n",
    "n_train_negative = int(train_share * len(negative_indices))\n",
    "\n",
    "train_indices = np.concatenate(\n",
    "    [\n",
    "        positive_indices[:n_train_positive], \n",
    "        negative_indices[:n_train_negative]\n",
    "    ], \n",
    "    axis=None)\n",
    "test_indices = np.concatenate(\n",
    "    [\n",
    "        positive_indices[n_train_positive:], \n",
    "        negative_indices[n_train_negative:]\n",
    "    ], \n",
    "    axis=None)\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138866396.4501971\n",
      "352578.7102546689\n",
      "924884.805999862\n",
      "4856963.899036874\n",
      "173523.70305159467\n",
      "84489.90593604438\n",
      "26773.477902543167\n",
      "363.4109548462469\n",
      "348.31436751412804\n",
      "343.4831582728844\n",
      "336.5929340539032\n",
      "336.08879765203363\n",
      "336.0467863574888\n",
      "336.0435547199029\n",
      "336.04332388865146\n",
      "336.04330849990174\n",
      "336.0433075381046\n",
      "336.0433074815296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_svm = LinearSVM()\n",
    "default_svm.fit(X_train, y_train)\n",
    "default_svm.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
