{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM example: Identifyng Chronic Kidney Disease\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through an example implementing a Support Vector Machine to tackle a medical classification problem. The data comes from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease), based on research by P. Soundarapandian, L. Jerlin Rubini, and P. Esweran.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Implementing a linear SVM\n",
    "1. Applying and tuning the model\n",
    "1. Implementing and applying kernel SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using both [NumPy](https://numpy.org/doc/stable/index.html) arrays and [pandas](https://pandas.pydata.org/docs/index.html) DataFrames, so we need to import these packages with their standard abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Linear SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Recall that a **Support Vector Machine** seeks to minimize classification errors while maximizing the width of the margin around the decision boundary, in hopes of increasing generalizability. More formally, we have a dataset of $n$ feature vectors $x^{(i)}$ and labels $y^{(i)}\\in {-1,1}$. We want to find a set of parameters ($\\theta, \\theta_0$) that mimimize the cost function:\n",
    "$$ J(\\theta , \\theta _0) = \\frac{1}{n} \\sum _{i=1}^{n} \\text {Loss}_ h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta _0 )) + \\frac{\\lambda }{2} \\mid \\mid \\theta \\mid \\mid ^2$$\n",
    "where\n",
    "$$\\text{Loss}_h(z)=\\begin{cases}0 & z \\ge 1 \\\\1-z & z < 1\\end{cases}$$\n",
    "and $\\lambda$ is the **regularization parameter**, determining the relative weight given to margin width (i.e. the inverse of the squared norm of $\\theta$) and accuracy (i.e. the `Loss` term). Higher $\\lambda$ focuses more on regularization by widening the margin.\n",
    "\n",
    "We estimate the parameters through **Stochastic Gradient Descent (SGD)**. This means we randomly select an observation $i$ and update the parameters as follows:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla _{\\theta } \\big [\\text {Loss}_ h(y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0) ) + \\frac{\\lambda }{2}\\mid \\mid \\theta \\mid \\mid ^2 \\big ]$$\n",
    "where $\\nabla$ is the learning rate. This can be a constant or can be adjusted over the training process. We will use the latter, based on the formula:\n",
    "$$ \\eta_{t+1} = \\frac {\\eta_t}{t^d} $$\n",
    "where $t$ is the training epoch, $\\eta_t$ is the learning rate for the $t$-th epoch, and $d$ is a decay parameter. \n",
    "\n",
    "If the case is placed correctly and outside the classification margin by the current parameters (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)\\ge 1$), the update is based strictly on the regularization term. So, the gradient is $\\lambda \\theta$.\n",
    "\n",
    "If there is a positive `Loss` (i.e. $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0)<1$), then the gradient is $-y^{(i)}x^{(i)} + \\lambda \\theta$.\n",
    "\n",
    "We continue cycling through the data, shuffling after each epoch, until the change in the cost remains below some threshold.\n",
    "\n",
    "Note that this algorithm only optimizes the parameters $\\theta$, and $\\theta_0$, while $\\lambda$, the initial $\\eta$, the learning rate decay parameter, and the convergence threshold are **hyperparameters** that must be set in advance.\n",
    "\n",
    "NOTE ABOUT THE OFFSET: rather than treating $\\theta_0$ as a separate parameter, it is possibe instead to prepend a `1` to every feature vector, so that the SVM trains $\\theta$ as a $D+1$ vector, where the first entry is equivalent to $\\theta_0$. This is what will be done below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The cell below implements a linear SVM as a class that mimics the syntax of the popular machine learning package [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
    "1. The model is first initialized with given (default or custom) hyperparameters. Here those are the regularization paramter `lam` (for $\\lambda$), the learning rate `lrate`, and the convergence `threshold`.\n",
    "1. Then, the model is trained on data using the method `.fit(X, y)`, where X contains the features vectors (observations as rows, features as columns) and y contains the labels. The model saves the trained parameters.\n",
    "1. After that, the model can be used to assign labels to any feature vectors with the method `.predict(X)`\n",
    "1. Finally, `.score(X, y)` calculates and returns the share of correctly predicted cases.\n",
    "\n",
    "The class below also contains three helper functions called by `.predict()`:\n",
    "1. `.cost()` calculates the cost function for the entire dataset, needed to check for convergence\n",
    "1. `.loss()` calculates and returns the Loss function for calculating the cost function, again as defined above\n",
    "1. `.train_epoch()` runs through the observations in random order one time, updating the parameters accordingly, and returning the resulting cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM():\n",
    "    def __init__(self, lam=1., lrate=1., decay=0.5, threshold=1e-4) -> None:\n",
    "        '''\n",
    "        Initialize the model with the given hyperparameters:\n",
    "        - `lam`: the regularization parameter lambda\n",
    "        - `lrate`: the learning rate\n",
    "        - `threshold`: the convergence threshold\n",
    "        '''\n",
    "        self.lam = lam\n",
    "        self.lrate = lrate\n",
    "        self.decay = decay\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "        self.max_iter = 1000\n",
    "\n",
    "    def loss(self, Xi, yi):\n",
    "        '''The Loss function for one observation'''\n",
    "        if self.theta is None:\n",
    "            raise ValueError('Cannot calculate loss without initializing theta')\n",
    "\n",
    "        agreement = yi*(self.theta @ Xi)\n",
    "        return 0 if agreement >= 1 else 1-agreement\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        '''Average cost for the whole dataset'''\n",
    "        loss_sum = 0\n",
    "        for Xi, yi in zip(X, y):\n",
    "            loss_sum += self.loss(Xi, yi)\n",
    "        return loss_sum/len(y) + self.lam/2 * (self.theta@self.theta)\n",
    "    \n",
    "    def train_epoch(self, X, y):\n",
    "        '''Cycles through cases in random order, updating theta. Returns the average cost after every update'''\n",
    "        order = np.arange(len(y)) \n",
    "        RNG.shuffle(order)\n",
    "        for i in order:\n",
    "            grad = self.lam * self.theta\n",
    "            grad -= y[i]*X[i] if self.loss(X[i], y[i]) > 0 else 0\n",
    "            self.theta -= self.lrate * grad\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Fits the model to the data (feature vectors X, labels y). To fit with an offset, prepend `1` to every feature vector'''\n",
    "        # initialize theta\n",
    "        self.theta = np.zeros(X.shape[-1])\n",
    "\n",
    "        last_cost = np.inf\n",
    "        for i in range(self.max_iter):\n",
    "            new_cost = self.train_epoch(X, y)\n",
    "            if np.abs(last_cost - new_cost) < self.threshold:\n",
    "                break\n",
    "            else:\n",
    "                last_cost = new_cost\n",
    "                self.lrate = self.lrate/(1+i)**self.decay\n",
    "        else:\n",
    "            print(f\"Maximum of {self.max_iter} reached without convergence\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predicts labels for feature vectors X'''\n",
    "        return sign(X @ self.theta)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y == y_pred)\n",
    "\n",
    "@np.vectorize\n",
    "def sign(z):\n",
    "    '''Converts values to labels, i.e. 1 and -1'''\n",
    "    return 1 if z>=0 else -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying and Tuning the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as in the file `ckd_data.csv`, and we can read it into a pandas DataFrame.\n",
    "\n",
    "The data has 400 rows. However, only the target label (`class`) has a full 400 observations: every feature has at least some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     391 non-null    float64\n",
      " 1   bp      388 non-null    float64\n",
      " 2   sg      353 non-null    float64\n",
      " 3   al      354 non-null    float64\n",
      " 4   su      351 non-null    float64\n",
      " 5   rbc     248 non-null    object \n",
      " 6   pc      335 non-null    object \n",
      " 7   pcc     396 non-null    object \n",
      " 8   ba      396 non-null    object \n",
      " 9   bgr     356 non-null    float64\n",
      " 10  bu      381 non-null    float64\n",
      " 11  sc      383 non-null    float64\n",
      " 12  sod     313 non-null    float64\n",
      " 13  pot     312 non-null    float64\n",
      " 14  hemo    348 non-null    float64\n",
      " 15  pcv     329 non-null    float64\n",
      " 16  wbcc    294 non-null    float64\n",
      " 17  rbcc    269 non-null    float64\n",
      " 18  htn     398 non-null    object \n",
      " 19  dm      398 non-null    object \n",
      " 20  cad     398 non-null    object \n",
      " 21  appet   399 non-null    object \n",
      " 22  pe      399 non-null    object \n",
      " 23  ane     399 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 81.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('ckd_data.csv', index_col=0)\n",
    "print(raw_data.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Model\n",
    "\n",
    "As a first pass, we can adopt the most conservative approach to the missing values: dropping any row that contained any. This results in 158 valid observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 3 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     158 non-null    float64\n",
      " 1   bp      158 non-null    float64\n",
      " 2   sg      158 non-null    float64\n",
      " 3   al      158 non-null    float64\n",
      " 4   su      158 non-null    float64\n",
      " 5   rbc     158 non-null    object \n",
      " 6   pc      158 non-null    object \n",
      " 7   pcc     158 non-null    object \n",
      " 8   ba      158 non-null    object \n",
      " 9   bgr     158 non-null    float64\n",
      " 10  bu      158 non-null    float64\n",
      " 11  sc      158 non-null    float64\n",
      " 12  sod     158 non-null    float64\n",
      " 13  pot     158 non-null    float64\n",
      " 14  hemo    158 non-null    float64\n",
      " 15  pcv     158 non-null    float64\n",
      " 16  wbcc    158 non-null    float64\n",
      " 17  rbcc    158 non-null    float64\n",
      " 18  htn     158 non-null    object \n",
      " 19  dm      158 non-null    object \n",
      " 20  cad     158 non-null    object \n",
      " 21  appet   158 non-null    object \n",
      " 22  pe      158 non-null    object \n",
      " 23  ane     158 non-null    object \n",
      " 24  class   158 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_dropNA = raw_data.dropna()\n",
    "data_dropNA.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the non-numeric columns (dtype object above) into a numeric format. As shown below, tt turns out that each of these features represent binary categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1\n",
      "rbc        normal    abnormal\n",
      "pc       abnormal      normal\n",
      "pcc       present  notpresent\n",
      "ba     notpresent     present\n",
      "htn           yes          no\n",
      "dm             no         yes\n",
      "cad            no         yes\n",
      "appet        poor        good\n",
      "pe            yes          no\n",
      "ane           yes          no\n",
      "class         ckd      notckd\n"
     ]
    }
   ],
   "source": [
    "print(data_dropNA.select_dtypes('object').apply(pd.unique).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can easily use binary encoding: replacing the categorical feature with that is 1 if the case falls within one category, otherwise 0. This can be done with the pandas method `.getdummies()`. Note that the parameter `drop_first=True` ensures that only one binary feature is created for each categorical feature.\n",
    "\n",
    "For the target label, we need to recode for the SVM algorithm: the positive label $+1$ applies to patients with chronic kidney disease, and the negative label $-1$ to those who do not.\n",
    "\n",
    "Finally, while the single pandas DataFrame is convenient to exploring the data, for training the model we can split it into feature vectors (X) and labels (y). We also add a 1 to each feature vector to act as an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame):\n",
    "    '''Encoded categorical data numerically and returns separate feature vectors and labels'''\n",
    "    df_copy = df.copy()\n",
    "    df_copy['offset'] = 1\n",
    "    X = pd.get_dummies(df_copy.drop(columns='class'), drop_first=True).to_numpy()\n",
    "    y = np.where(df_copy['class'] == 'ckd', 1., -1.)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(data_dropNA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not yet ready to train the model, however. We need to split our data into training and test sets. The training data is used to train the model, while the test data is used to assess its generalizable performance. We will set aside $\\frac{1}{4}$ of the latter as the test data. This is done randomly, with one adjustment: we want to preserve the rough balance of positively and negatively labeled cases in the training and testing data.\n",
    "\n",
    "We thus will end up with 4 numpy arrays: `X_train`, `X_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y:np.ndarray, train_size:float):\n",
    "    '''Randomly splits arrays into training and test sets, stratifying on the values of `y`'''\n",
    "    positive_indices = np.flatnonzero(y == 1)\n",
    "    RNG.shuffle(positive_indices)\n",
    "    n_train_positive = int(train_size * len(positive_indices))\n",
    "\n",
    "    negative_indices = np.flatnonzero(y == -1)\n",
    "    RNG.shuffle(negative_indices)\n",
    "    n_train_negative = int(train_size * len(negative_indices))\n",
    "\n",
    "    train_indices = np.concatenate(\n",
    "        [\n",
    "            positive_indices[:n_train_positive], \n",
    "            negative_indices[:n_train_negative]\n",
    "        ], \n",
    "        axis=None)\n",
    "    test_indices = np.concatenate(\n",
    "        [\n",
    "            positive_indices[n_train_positive:], \n",
    "            negative_indices[n_train_negative:]\n",
    "        ], \n",
    "        axis=None)\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 25) (40, 25) (118,) (40,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.75)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note here is that between dropping missing values and splitting the data, we are left with rather few observations.\n",
    "\n",
    "The cells below fit the model and report on its classification performance in three ways:\n",
    "1. The share of correctly labeled observations in the training data.\n",
    "1. The share of correctly labeled observations in the testing data.\n",
    "1. More fine-grained measures, broken down by label, for the testing data. Precision is the share of predictions that are correct, while recall is the share of members of the class that are correctly labeled by the classifier.\n",
    "\n",
    "There are two things to note here:\n",
    "1. Because the labels are imbalanced (i.e. there are many more negative than positive cases), the average accuracy does not fully capture the performance across the two labels. In particular, the precision and recall for the positive cases is usually noticeably lower.\n",
    "1. With so little data, the particular random split between training and testing data substantially impacts the model. To see this for yourself, rerun this notebook a few times and look at how the metrics change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training accuracy:  0.9576271186440678\n",
      "Average test accuracy:  0.975\n"
     ]
    }
   ],
   "source": [
    "default_svm = LinearSVM()\n",
    "default_svm.fit(X_train, y_train)\n",
    "print('Average training accuracy: ', default_svm.score(X_train, y_train))\n",
    "print('Average test accuracy: ', default_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.97      1.00      0.98        29\n",
      "         1.0       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.95      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = default_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     400 non-null    float64\n",
      " 1   bp      400 non-null    float64\n",
      " 2   sg      400 non-null    float64\n",
      " 3   al      400 non-null    float64\n",
      " 4   su      400 non-null    float64\n",
      " 5   rbc     400 non-null    object \n",
      " 6   pc      400 non-null    object \n",
      " 7   pcc     400 non-null    object \n",
      " 8   ba      400 non-null    object \n",
      " 9   bgr     400 non-null    float64\n",
      " 10  bu      400 non-null    float64\n",
      " 11  sc      400 non-null    float64\n",
      " 12  sod     400 non-null    float64\n",
      " 13  pot     400 non-null    float64\n",
      " 14  hemo    400 non-null    float64\n",
      " 15  pcv     400 non-null    float64\n",
      " 16  wbcc    400 non-null    float64\n",
      " 17  rbcc    400 non-null    float64\n",
      " 18  htn     400 non-null    object \n",
      " 19  dm      400 non-null    object \n",
      " 20  cad     400 non-null    object \n",
      " 21  appet   400 non-null    object \n",
      " 22  pe      400 non-null    object \n",
      " 23  ane     400 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 97.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_fill = raw_data.select_dtypes('object').mode().iloc[0]\n",
    "num_fill = raw_data.select_dtypes('float').mean().iloc[0]\n",
    "data_filled = raw_data.fillna(cat_fill)\n",
    "data_filled.fillna(num_fill, inplace=True)\n",
    "data_filled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8595317725752508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.82      0.82      0.82        38\n",
      "         1.0       0.89      0.89      0.89        63\n",
      "\n",
      "    accuracy                           0.86       101\n",
      "   macro avg       0.85      0.85      0.85       101\n",
      "weighted avg       0.86      0.86      0.86       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_data(data_filled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.75)\n",
    "svm = LinearSVM()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Training accuracy: ', svm.score(X_train, y_train))\n",
    "y_pred = svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
